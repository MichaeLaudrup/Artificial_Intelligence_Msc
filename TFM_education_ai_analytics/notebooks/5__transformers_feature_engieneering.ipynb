{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de71e455",
   "metadata": {},
   "source": [
    "# ðŸ§± Base Notebook: Transformers Feature Engineering\n",
    "\n",
    "Este notebook prepara datasets para Transformer por horizonte (`W12`, `W18`, `W24`) uniendo:\n",
    "- features base de `3_features` (day0 opcional),\n",
    "- salidas de clustering (`p_cluster_*`, `confidence`, `entropy`),\n",
    "- target final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Imports + Config\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from educational_ai_analytics.config import (\n",
    "        FEATURES_DATA_DIR, EMBEDDINGS_DATA_DIR, DATA_DIR, W_WINDOWS\n",
    "    )\n",
    "except Exception:\n",
    "    ROOT = Path('/workspace/TFM_education_ai_analytics')\n",
    "    DATA_DIR = ROOT / 'data'\n",
    "    FEATURES_DATA_DIR = DATA_DIR / '3_features'\n",
    "    EMBEDDINGS_DATA_DIR = DATA_DIR / '4_embeddings'\n",
    "    W_WINDOWS = [12, 18, 24]\n",
    "\n",
    "OUT_ROOT = DATA_DIR / '6_transformer_features'\n",
    "HORIZONS = sorted([int(w) for w in W_WINDOWS])\n",
    "SPLITS = ['training', 'validation', 'test']\n",
    "INCLUDE_DAY0 = True\n",
    "SEG_FILE = 'segmentation_gmm_ae.csv'\n",
    "TARGET_FILE = 'target.csv'\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print('âœ… Config cargada')\n",
    "print('Horizontes:', HORIZONS)\n",
    "print('Output:', OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Helpers\n",
    "# =========================\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    return pd.read_csv(path, index_col=0).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "def load_target(split: str) -> pd.DataFrame:\n",
    "    tgt = safe_read_csv(FEATURES_DATA_DIR / split / TARGET_FILE)\n",
    "    if tgt.empty:\n",
    "        raise FileNotFoundError(f'No target found for split={split}')\n",
    "    return tgt\n",
    "\n",
    "def load_day0(split: str) -> pd.DataFrame:\n",
    "    return safe_read_csv(FEATURES_DATA_DIR / split / 'day0_static_features.csv')\n",
    "\n",
    "def load_seg(split: str, w: int) -> pd.DataFrame:\n",
    "    p = EMBEDDINGS_DATA_DIR / split / f'upto_w{w:02d}' / SEG_FILE\n",
    "    seg = safe_read_csv(p)\n",
    "    if seg.empty:\n",
    "        return seg\n",
    "    cols = [c for c in seg.columns if c.startswith('p_cluster_')]\n",
    "    extra = [c for c in ['cluster_id', 'confidence', 'entropy', 'entropy_norm'] if c in seg.columns]\n",
    "    keep = cols + extra\n",
    "    seg = seg[keep].copy()\n",
    "    seg.columns = [f'{c}_w{w:02d}' for c in seg.columns]\n",
    "    return seg\n",
    "\n",
    "def build_horizon_table(split: str, horizon: int, include_day0: bool = True) -> pd.DataFrame:\n",
    "    windows = [w for w in HORIZONS if w <= horizon]\n",
    "    tgt = load_target(split)\n",
    "\n",
    "    base = tgt[['final_result']].copy()\n",
    "    if include_day0:\n",
    "        day0 = load_day0(split)\n",
    "        if not day0.empty:\n",
    "            day0 = day0.add_prefix('d0_')\n",
    "            base = base.join(day0, how='inner')\n",
    "\n",
    "    for w in windows:\n",
    "        seg = load_seg(split, w)\n",
    "        if seg.empty:\n",
    "            print(f'âš ï¸ split={split} W{w}: segmentation vacÃ­a/no encontrada')\n",
    "            continue\n",
    "        base = base.join(seg, how='inner')\n",
    "\n",
    "    base = base.dropna()\n",
    "    return base\n",
    "\n",
    "def export_horizon_dataset(df: pd.DataFrame, split: str, horizon: int):\n",
    "    out_dir = OUT_ROOT / split / f'w{horizon:02d}'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    y = df['final_result'].astype(int).values\n",
    "    feature_df = df.drop(columns=['final_result'])\n",
    "\n",
    "    prob_cols = [c for c in feature_df.columns if c.startswith('p_cluster_')]\n",
    "    static_cols = [c for c in feature_df.columns if c not in prob_cols]\n",
    "\n",
    "    windows = [w for w in HORIZONS if w <= horizon]\n",
    "    seq_steps = []\n",
    "    for w in windows:\n",
    "        step_cols = [c for c in prob_cols if c.endswith(f'_w{w:02d}')]\n",
    "        if step_cols:\n",
    "            seq_steps.append(feature_df[step_cols].values.astype(np.float32))\n",
    "\n",
    "    if len(seq_steps) == 0:\n",
    "        X_seq = np.zeros((len(feature_df), 1, 1), dtype=np.float32)\n",
    "    else:\n",
    "        X_seq = np.stack(seq_steps, axis=1)\n",
    "\n",
    "    X_static = feature_df[static_cols].values.astype(np.float32) if len(static_cols) else np.zeros((len(feature_df), 0), dtype=np.float32)\n",
    "\n",
    "    np.save(out_dir / 'X_seq.npy', X_seq)\n",
    "    np.save(out_dir / 'X_static.npy', X_static)\n",
    "    np.save(out_dir / 'y.npy', y)\n",
    "\n",
    "    feature_df.to_csv(out_dir / 'features_flat.csv')\n",
    "    pd.DataFrame(index=df.index).to_csv(out_dir / 'index.csv')\n",
    "\n",
    "    meta = {\n",
    "        'split': split,\n",
    "        'horizon': int(horizon),\n",
    "        'n_samples': int(len(df)),\n",
    "        'seq_shape': list(X_seq.shape),\n",
    "        'static_shape': list(X_static.shape),\n",
    "        'windows_used': windows,\n",
    "        'include_day0': bool(any(c.startswith('d0_') for c in static_cols)),\n",
    "        'n_prob_features_flat': int(len(prob_cols)),\n",
    "        'n_static_features': int(len(static_cols))\n",
    "    }\n",
    "    (out_dir / 'meta.json').write_text(json.dumps(meta, indent=2, ensure_ascii=False))\n",
    "\n",
    "    print(f'ðŸ’¾ Exportado {split} W{horizon}: N={len(df)} | seq={X_seq.shape} | static={X_static.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Build all datasets\n",
    "# =========================\n",
    "for split in SPLITS:\n",
    "    for horizon in HORIZONS:\n",
    "        try:\n",
    "            df_h = build_horizon_table(split=split, horizon=horizon, include_day0=INCLUDE_DAY0)\n",
    "            if df_h.empty:\n",
    "                print(f'âš ï¸ VacÃ­o: split={split}, W={horizon}')\n",
    "                continue\n",
    "            export_horizon_dataset(df_h, split=split, horizon=horizon)\n",
    "        except Exception as e:\n",
    "            print(f'âŒ Error split={split}, W={horizon}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) Quick sanity check\n",
    "# =========================\n",
    "rows = []\n",
    "for split in SPLITS:\n",
    "    for horizon in HORIZONS:\n",
    "        meta_path = OUT_ROOT / split / f'w{horizon:02d}' / 'meta.json'\n",
    "        if meta_path.exists():\n",
    "            m = json.loads(meta_path.read_text())\n",
    "            rows.append(m)\n",
    "\n",
    "pd.DataFrame(rows) if rows else print('No se encontraron metadatos exportados.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
