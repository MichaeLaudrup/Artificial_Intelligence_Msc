{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de71e455",
   "metadata": {},
   "source": [
    "# ðŸ§± Base Notebook: Transformers Feature Engineering\n",
    "\n",
    "Este notebook prepara datasets para Transformer por horizonte (`W12`, `W18`, `W24`) uniendo:\n",
    "- features base de `3_features` (day0 opcional),\n",
    "- salidas de clustering (`p_cluster_*`, `confidence`, `entropy`),\n",
    "- target final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b70b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Config cargada\n",
      "Horizontes: [12, 18, 24]\n",
      "Output: /workspace/TFM_education_ai_analytics/data/6_transformer_features\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) Imports + Config\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from educational_ai_analytics.config import (\n",
    "        FEATURES_DATA_DIR, EMBEDDINGS_DATA_DIR, DATA_DIR, W_WINDOWS\n",
    "    )\n",
    "except Exception:\n",
    "    ROOT = Path('/workspace/TFM_education_ai_analytics')\n",
    "    DATA_DIR = ROOT / 'data'\n",
    "    FEATURES_DATA_DIR = DATA_DIR / '3_features'\n",
    "    EMBEDDINGS_DATA_DIR = DATA_DIR / '4_embeddings'\n",
    "    W_WINDOWS = [12, 18, 24]\n",
    "\n",
    "OUT_ROOT = DATA_DIR / '6_transformer_features'\n",
    "HORIZONS = sorted([int(w) for w in W_WINDOWS])\n",
    "SPLITS = ['training', 'validation', 'test']\n",
    "INCLUDE_DAY0 = True\n",
    "SEG_FILE = 'segmentation_gmm_ae.csv'\n",
    "TARGET_FILE = 'target.csv'\n",
    "\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "print('âœ… Config cargada')\n",
    "print('Horizontes:', HORIZONS)\n",
    "print('Output:', OUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Helpers\n",
    "# =========================\n",
    "def safe_read_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        return pd.DataFrame()\n",
    "    return pd.read_csv(path, index_col=0).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "def load_target(split: str) -> pd.DataFrame:\n",
    "    tgt = safe_read_csv(FEATURES_DATA_DIR / split / TARGET_FILE)\n",
    "    if tgt.empty:\n",
    "        raise FileNotFoundError(f'No target found for split={split}')\n",
    "    return tgt\n",
    "\n",
    "def load_day0(split: str) -> pd.DataFrame:\n",
    "    return safe_read_csv(FEATURES_DATA_DIR / split / 'day0_static_features.csv')\n",
    "\n",
    "def load_seg(split: str, w: int) -> pd.DataFrame:\n",
    "    p = EMBEDDINGS_DATA_DIR / split / f'upto_w{w:02d}' / SEG_FILE\n",
    "    seg = safe_read_csv(p)\n",
    "    if seg.empty:\n",
    "        return seg\n",
    "    cols = [c for c in seg.columns if c.startswith('p_cluster_')]\n",
    "    extra = [c for c in ['cluster_id', 'confidence', 'entropy', 'entropy_norm'] if c in seg.columns]\n",
    "    keep = cols + extra\n",
    "    seg = seg[keep].copy()\n",
    "    seg.columns = [f'{c}_w{w:02d}' for c in seg.columns]\n",
    "    return seg\n",
    "\n",
    "def build_horizon_table(split: str, horizon: int, include_day0: bool = True) -> pd.DataFrame:\n",
    "    windows = [w for w in HORIZONS if w <= horizon]\n",
    "    tgt = load_target(split)\n",
    "\n",
    "    base = tgt[['final_result']].copy()\n",
    "    if include_day0:\n",
    "        day0 = load_day0(split)\n",
    "        if not day0.empty:\n",
    "            day0 = day0.add_prefix('d0_')\n",
    "            base = base.join(day0, how='inner')\n",
    "\n",
    "    for w in windows:\n",
    "        seg = load_seg(split, w)\n",
    "        if seg.empty:\n",
    "            print(f'âš ï¸ split={split} W{w}: segmentation vacÃ­a/no encontrada')\n",
    "            continue\n",
    "        base = base.join(seg, how='inner')\n",
    "\n",
    "    base = base.dropna()\n",
    "    return base\n",
    "\n",
    "def export_horizon_dataset(df: pd.DataFrame, split: str, horizon: int):\n",
    "    out_dir = OUT_ROOT / split / f'w{horizon:02d}'\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    y = df['final_result'].astype(int).values\n",
    "    feature_df = df.drop(columns=['final_result'])\n",
    "\n",
    "    prob_cols = [c for c in feature_df.columns if c.startswith('p_cluster_')]\n",
    "    static_cols = [c for c in feature_df.columns if c not in prob_cols]\n",
    "\n",
    "    windows = [w for w in HORIZONS if w <= horizon]\n",
    "    seq_steps = []\n",
    "    for w in windows:\n",
    "        step_cols = [c for c in prob_cols if c.endswith(f'_w{w:02d}')]\n",
    "        if step_cols:\n",
    "            seq_steps.append(feature_df[step_cols].values.astype(np.float32))\n",
    "\n",
    "    if len(seq_steps) == 0:\n",
    "        X_seq = np.zeros((len(feature_df), 1, 1), dtype=np.float32)\n",
    "    else:\n",
    "        X_seq = np.stack(seq_steps, axis=1)\n",
    "\n",
    "    X_static = feature_df[static_cols].values.astype(np.float32) if len(static_cols) else np.zeros((len(feature_df), 0), dtype=np.float32)\n",
    "\n",
    "    np.save(out_dir / 'X_seq.npy', X_seq)\n",
    "    np.save(out_dir / 'X_static.npy', X_static)\n",
    "    np.save(out_dir / 'y.npy', y)\n",
    "\n",
    "    feature_df.to_csv(out_dir / 'features_flat.csv')\n",
    "    pd.DataFrame(index=df.index).to_csv(out_dir / 'index.csv')\n",
    "\n",
    "    meta = {\n",
    "        'split': split,\n",
    "        'horizon': int(horizon),\n",
    "        'n_samples': int(len(df)),\n",
    "        'seq_shape': list(X_seq.shape),\n",
    "        'static_shape': list(X_static.shape),\n",
    "        'windows_used': windows,\n",
    "        'include_day0': bool(any(c.startswith('d0_') for c in static_cols)),\n",
    "        'n_prob_features_flat': int(len(prob_cols)),\n",
    "        'n_static_features': int(len(static_cols))\n",
    "    }\n",
    "    (out_dir / 'meta.json').write_text(json.dumps(meta, indent=2, ensure_ascii=False))\n",
    "\n",
    "    print(f'ðŸ’¾ Exportado {split} W{horizon}: N={len(df)} | seq={X_seq.shape} | static={X_static.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d613bbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 17:45:51.241461: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        pos = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]  # (L, 1)\n",
    "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]    # (1, D)\n",
    "\n",
    "        angle_rates = 1.0 / tf.pow(10000.0, (2.0 * tf.floor(i / 2.0)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        # even -> sin, odd -> cos\n",
    "        sin_part = tf.sin(angle_rads[:, 0::2])\n",
    "        cos_part = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pe = tf.concat([sin_part, cos_part], axis=-1)\n",
    "        pe = pe[tf.newaxis, ...]  # (1, L, D)\n",
    "        self.pe = tf.cast(pe, tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, d_model: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add1 = layers.Add()\n",
    "\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add2 = layers.Add()\n",
    "\n",
    "    def call(self, x, training=False, attention_mask=None):\n",
    "        attn_out = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        x = self.add1([x, attn_out])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ffn_out = self.ffn(x, training=training)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        x = self.add2([x, ffn_out])\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLULayer(layers.Layer):\n",
    "    # h(X) = (XW) âŠ— sigmoid(XV)\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.proj = layers.Dense(2 * d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.proj(x)\n",
    "        a, b = tf.split(z, num_or_size_splits=2, axis=-1)\n",
    "        return a * tf.sigmoid(b)\n",
    "\n",
    "\n",
    "class GLUTransformerClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_d: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        dropout: float,\n",
    "        num_classes: int,\n",
    "        num_layers: int = 2,\n",
    "        max_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if latent_d % num_heads != 0:\n",
    "            raise ValueError(\"latent_d debe ser divisible por num_heads\")\n",
    "\n",
    "        self.input_proj = layers.Dense(latent_d)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=latent_d, max_len=max_len)\n",
    "        self.input_dropout = layers.Dropout(dropout)\n",
    "\n",
    "        self.encoders = [\n",
    "            TransformerEncoderBlock(\n",
    "                d_model=latent_d,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.glu = GLULayer(d_model=latent_d)\n",
    "        self.norm_out = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.head = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_d, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # inputs: (B, W, F)\n",
    "        # mask:   (B, W) con 1/0 o bool (opcional)\n",
    "\n",
    "        x = self.input_proj(inputs)          # (B, W, D)\n",
    "        x = self.pos_encoding(x)             # (B, W, D)\n",
    "        x = self.input_dropout(x, training=training)\n",
    "\n",
    "        # MÃ¡scara para MHA: (B, 1, W), broadcast a (B, W, W)\n",
    "        attn_mask = None\n",
    "        if mask is not None:\n",
    "            attn_mask = tf.cast(mask[:, tf.newaxis, :], tf.bool)\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, training=training, attention_mask=attn_mask)\n",
    "\n",
    "        x = self.glu(x)\n",
    "        x = self.norm_out(x)\n",
    "\n",
    "        # Masked pooling temporal\n",
    "        if mask is not None:\n",
    "            m = tf.cast(mask, x.dtype)[:, :, tf.newaxis]     # (B, W, 1)\n",
    "            x_sum = tf.reduce_sum(x * m, axis=1)             # (B, D)\n",
    "            denom = tf.reduce_sum(m, axis=1) + 1e-8          # (B, 1)\n",
    "            pooled = x_sum / denom\n",
    "        else:\n",
    "            pooled = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "        return self.head(pooled)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
