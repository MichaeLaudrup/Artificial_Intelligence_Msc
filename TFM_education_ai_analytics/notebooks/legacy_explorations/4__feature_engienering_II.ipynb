{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6606f5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Paths OK\n",
      "PROJECT_DIR: /workspace/TFM_education_ai_analytics\n",
      "OUT_DIR: /workspace/TFM_education_ai_analytics/data/6_transformer_features\n",
      "WEEKS_LIST: [12, 16, 20, 24]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA 1] Setup + Paths + Config\n",
    "# =========================\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reproducibilidad (para sampling/plots si lo usas)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# --- Proyecto ---\n",
    "PROJECT_DIR = Path(\"/workspace/TFM_education_ai_analytics\")\n",
    "\n",
    "# --- Entradas base (ya generadas por tus pipelines) ---\n",
    "PROCESSED_DIR = PROJECT_DIR / \"data/2_processed\"          # students/interactions/assessments por split\n",
    "FEATURES_DIR  = PROJECT_DIR / \"data/3_features\"           # engineered_features.csv + target.csv por split (FE1)\n",
    "EMB_DIR       = PROJECT_DIR / \"data/4_embeddings\"         # segmentation_gmm_ae.csv por split\n",
    "\n",
    "# --- Salidas para el Transformer (nuevo) ---\n",
    "OUT_DIR = PROJECT_DIR / \"data/6_transformer_features\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Splits\n",
    "SPLITS = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "# Ventanas de semanas (como en el paper, adaptado a tu caso)\n",
    "WEEKS_LIST = [12, 16, 20, 24]\n",
    "\n",
    "print(\"âœ… Paths OK\")\n",
    "print(\"PROJECT_DIR:\", PROJECT_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"WEEKS_LIST:\", WEEKS_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe9e2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Checking split: training\n",
      "  âœ… processed_students: students.csv\n",
      "  âœ… processed_interactions: interactions.csv\n",
      "  âœ… processed_assessments: assessments.csv\n",
      "  âœ… target: target.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "âŒ No existe engineered (training): /workspace/TFM_education_ai_analytics/data/3_features/training/engineered_features.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, fn \u001b[38;5;129;01min\u001b[39;00m expected.items():\n\u001b[32m     25\u001b[39m         p = fn(split)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[43mcheck_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m (\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  âœ… \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 2) Cargar TRAIN para inspecciÃ³n rÃ¡pida\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcheck_exists\u001b[39m\u001b[34m(path, label)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_exists\u001b[39m(path: Path, label: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâŒ No existe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: âŒ No existe engineered (training): /workspace/TFM_education_ai_analytics/data/3_features/training/engineered_features.csv"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA 2] Sanity check + carga rÃ¡pida (TRAIN)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "def check_exists(path: Path, label: str):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"âŒ No existe {label}: {path}\")\n",
    "    return True\n",
    "\n",
    "# --- Archivos esperados por split ---\n",
    "expected = {\n",
    "    \"processed_students\": lambda split: PROCESSED_DIR / split / \"students.csv\",\n",
    "    \"processed_interactions\": lambda split: PROCESSED_DIR / split / \"interactions.csv\",\n",
    "    \"processed_assessments\": lambda split: PROCESSED_DIR / split / \"assessments.csv\",\n",
    "    \"target\": lambda split: FEATURES_DIR / split / \"target.csv\",\n",
    "    \"engineered\": lambda split: FEATURES_DIR / split / \"engineered_features.csv\",\n",
    "\"segmentation\": lambda split: (PROJECT_DIR / \"data/5_students_segmented\") / split / \"segmentation_gmm_ae.csv\",\n",
    "}\n",
    "\n",
    "# 1) Comprobar existencia\n",
    "for split in SPLITS:\n",
    "    print(f\"\\nðŸ”Ž Checking split: {split}\")\n",
    "    for name, fn in expected.items():\n",
    "        p = fn(split)\n",
    "        check_exists(p, f\"{name} ({split})\")\n",
    "        print(f\"  âœ… {name}: {p.name}\")\n",
    "\n",
    "# 2) Cargar TRAIN para inspecciÃ³n rÃ¡pida\n",
    "train_students = pd.read_csv(expected[\"processed_students\"](\"training\"))\n",
    "train_interactions = pd.read_csv(expected[\"processed_interactions\"](\"training\"))\n",
    "train_target = pd.read_csv(expected[\"target\"](\"training\"), index_col=0)\n",
    "train_seg = pd.read_csv(expected[\"segmentation\"](\"training\"), index_col=0)\n",
    "\n",
    "print(\"\\nðŸ“Œ TRAIN shapes:\")\n",
    "print(\"students:\", train_students.shape)\n",
    "print(\"interactions:\", train_interactions.shape)\n",
    "print(\"target:\", train_target.shape)\n",
    "print(\"segmentation:\", train_seg.shape)\n",
    "\n",
    "print(\"\\nðŸ“Œ TRAIN columns (heads):\")\n",
    "display(train_students.head(3))\n",
    "display(train_interactions.head(3))\n",
    "display(train_target.head(3))\n",
    "display(train_seg.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ab9b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PROJECT_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m EXCLUDE_COLS = [\u001b[33m\"\u001b[39m\u001b[33mfinal_result\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdate_unregistration\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Paths FE2 (ya generados previamente)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m OUT_DIR = \u001b[43mPROJECT_DIR\u001b[49m / \u001b[33m\"\u001b[39m\u001b[33mdata/6_transformer_features\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# asegÃºrate que PROJECT_DIR existe\u001b[39;00m\n\u001b[32m     21\u001b[39m SPLITS = [\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ---- HELPERS ----\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'PROJECT_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA INDEPENDIENTE] EstÃ¡ticas desde students.csv (safe) + schema TRAIN + scaler global + merge con FE2 + guardado\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---- CONFIG ----\n",
    "W = 12  # cambia si quieres otra ventana\n",
    "\n",
    "# Columnas estÃ¡ticas \"safe\" (inicio de curso)\n",
    "STATIC_CATEGORICAL = [\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"disability\"]\n",
    "STATIC_NUMERIC = [\"num_of_prev_attempts\", \"studied_credits\", \"date_registration\", \"module_presentation_length\"]\n",
    "\n",
    "# Nunca usar (leakage/target)\n",
    "EXCLUDE_COLS = [\"final_result\", \"date_unregistration\"]\n",
    "\n",
    "# Paths FE2 (ya generados previamente)\n",
    "OUT_DIR = PROJECT_DIR / \"data/6_transformer_features\"  # asegÃºrate que PROJECT_DIR existe\n",
    "SPLITS = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "\n",
    "# ---- HELPERS ----\n",
    "def _require(name: str):\n",
    "    if name not in globals():\n",
    "        raise NameError(\n",
    "            f\"Falta la variable '{name}' en el notebook. \"\n",
    "            f\"AsegÃºrate de haber cargado los dataframes de students y FE2 antes de ejecutar esta celda.\"\n",
    "        )\n",
    "\n",
    "def build_unique_id_from_students(df_students: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Construye unique_id = id_student_code_module_code_presentation\n",
    "    (basado en tu convenciÃ³n del proyecto)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df_students[\"id_student\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_students[\"code_module\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_students[\"code_presentation\"].astype(str)\n",
    "    )\n",
    "\n",
    "def build_static_from_students(df_students: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Devuelve X_static con index=unique_id.\n",
    "    - OHE para categÃ³ricas\n",
    "    - numÃ©ricas como float (sin escalar aquÃ­)\n",
    "    - excluye columnas peligrosas\n",
    "    \"\"\"\n",
    "    df = df_students.copy()\n",
    "\n",
    "    # Excluir leakage/target-like si existieran\n",
    "    for c in EXCLUDE_COLS:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    # unique_id\n",
    "    if \"unique_id\" not in df.columns:\n",
    "        df[\"unique_id\"] = build_unique_id_from_students(df)\n",
    "\n",
    "    df = df.set_index(\"unique_id\")\n",
    "\n",
    "    # NumÃ©ricas\n",
    "    num_cols = [c for c in STATIC_NUMERIC if c in df.columns]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "\n",
    "    # CategÃ³ricas\n",
    "    cat_cols = [c for c in STATIC_CATEGORICAL if c in df.columns]\n",
    "    X_cat = pd.get_dummies(\n",
    "        df[cat_cols].fillna(\"UNKNOWN\").astype(str),\n",
    "        prefix=cat_cols,\n",
    "        drop_first=False,\n",
    "    )\n",
    "\n",
    "    # Unir\n",
    "    X_num = df[num_cols].copy()\n",
    "    X = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X.index.name = \"unique_id\"\n",
    "    return X\n",
    "\n",
    "\n",
    "def apply_static_schema_and_scaler(\n",
    "    df_students: pd.DataFrame,\n",
    "    schema_cols: list[str],\n",
    "    scaler: StandardScaler,\n",
    "    numeric_cols_present_in_schema: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Construye estÃ¡ticas, las reindexa a schema TRAIN, y aplica scaler solo a numÃ©ricas.\n",
    "    \"\"\"\n",
    "    X = build_static_from_students(df_students)\n",
    "    X = X.reindex(columns=schema_cols, fill_value=0.0)\n",
    "\n",
    "    cols_to_scale = [c for c in numeric_cols_present_in_schema if c in X.columns]\n",
    "    if cols_to_scale:\n",
    "        X[cols_to_scale] = scaler.transform(X[cols_to_scale])\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return X\n",
    "\n",
    "\n",
    "# ---- REQUIRE INPUTS ----\n",
    "# Necesitas estos dataframes ya cargados en tu notebook:\n",
    "# - train_students, val_students, test_students  (desde data/2_processed/<split>/students.csv)\n",
    "# - (opciÃ³n A) X_train_w12, X_val_w12, X_test_w12 (FE2 en memoria)\n",
    "#   o (opciÃ³n B) wXX_X_seq.csv guardados en disco\n",
    "for v in [\"train_students\", \"val_students\", \"test_students\"]:\n",
    "    _require(v)\n",
    "\n",
    "# Detectar si tienes FE2 en memoria; si no, lo cargamos desde disco\n",
    "have_in_memory = all([f\"X_{sp}_w{W}\" in globals() for sp in [\"train\", \"val\", \"test\"]])\n",
    "\n",
    "def load_fe2_seq(split: str) -> pd.DataFrame:\n",
    "    path = OUT_DIR / split / f\"w{W:02d}_X_seq.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"No existe FE2 secuencias: {path}\")\n",
    "    return pd.read_csv(path, index_col=0)\n",
    "\n",
    "if have_in_memory:\n",
    "    X_train_seq = globals()[f\"X_train_w{W}\"]\n",
    "    X_val_seq   = globals()[f\"X_val_w{W}\"]\n",
    "    X_test_seq  = globals()[f\"X_test_w{W}\"]\n",
    "else:\n",
    "    X_train_seq = load_fe2_seq(\"training\")\n",
    "    X_val_seq   = load_fe2_seq(\"validation\")\n",
    "    X_test_seq  = load_fe2_seq(\"test\")\n",
    "\n",
    "\n",
    "# ---- BUILD STATIC (TRAIN schema + scaler) ----\n",
    "X_static_train = build_static_from_students(train_students)\n",
    "\n",
    "static_schema = X_static_train.columns.tolist()\n",
    "numeric_cols_in_schema = [c for c in STATIC_NUMERIC if c in X_static_train.columns]\n",
    "\n",
    "scaler_static = StandardScaler()\n",
    "if numeric_cols_in_schema:\n",
    "    X_static_train[numeric_cols_in_schema] = scaler_static.fit_transform(X_static_train[numeric_cols_in_schema])\n",
    "\n",
    "X_static_val = apply_static_schema_and_scaler(\n",
    "    val_students,\n",
    "    schema_cols=static_schema,\n",
    "    scaler=scaler_static,\n",
    "    numeric_cols_present_in_schema=numeric_cols_in_schema,\n",
    ")\n",
    "X_static_test = apply_static_schema_and_scaler(\n",
    "    test_students,\n",
    "    schema_cols=static_schema,\n",
    "    scaler=scaler_static,\n",
    "    numeric_cols_present_in_schema=numeric_cols_in_schema,\n",
    ")\n",
    "\n",
    "print(\"âœ… static shapes:\", X_static_train.shape, X_static_val.shape, X_static_test.shape)\n",
    "\n",
    "\n",
    "# ---- MERGE STATIC + FE2 SEQ ----\n",
    "X_train_hybrid = X_train_seq.join(X_static_train, how=\"left\").fillna(0.0)\n",
    "X_val_hybrid   = X_val_seq.join(X_static_val,   how=\"left\").fillna(0.0)\n",
    "X_test_hybrid  = X_test_seq.join(X_static_test, how=\"left\").fillna(0.0)\n",
    "\n",
    "print(\"âœ… hybrid shapes:\", X_train_hybrid.shape, X_val_hybrid.shape, X_test_hybrid.shape)\n",
    "\n",
    "\n",
    "# ---- SAVE ----\n",
    "def save_hybrid(split: str, df: pd.DataFrame):\n",
    "    out_dir = OUT_DIR / split\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    out_path = out_dir / f\"w{W:02d}_X_hybrid.csv\"\n",
    "    df.to_csv(out_path)\n",
    "    print(\"ðŸ’¾ guardado:\", out_path)\n",
    "\n",
    "save_hybrid(\"training\", X_train_hybrid)\n",
    "save_hybrid(\"validation\", X_val_hybrid)\n",
    "save_hybrid(\"test\", X_test_hybrid)\n",
    "\n",
    "# Preview\n",
    "display(X_train_hybrid.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22019dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… X_train_w12 (seq+prestart+norm): (22785, 236)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clicks_dataplus_w00</th>\n",
       "      <th>clicks_dataplus_w01</th>\n",
       "      <th>clicks_dataplus_w02</th>\n",
       "      <th>clicks_dataplus_w03</th>\n",
       "      <th>clicks_dataplus_w04</th>\n",
       "      <th>clicks_dataplus_w05</th>\n",
       "      <th>clicks_dataplus_w06</th>\n",
       "      <th>clicks_dataplus_w07</th>\n",
       "      <th>clicks_dataplus_w08</th>\n",
       "      <th>clicks_dataplus_w09</th>\n",
       "      <th>...</th>\n",
       "      <th>total_clicks_w11</th>\n",
       "      <th>active_weeks_uptoW</th>\n",
       "      <th>early_ratio_uptoW</th>\n",
       "      <th>late_ratio_uptoW</th>\n",
       "      <th>prestart_clicks_total</th>\n",
       "      <th>prestart_active_days</th>\n",
       "      <th>prestart_active_weeks</th>\n",
       "      <th>prestart_earliest_day</th>\n",
       "      <th>investigated_platform</th>\n",
       "      <th>prestart_ratio_vs_uptoW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11391_AAA_2013J</th>\n",
       "      <td>-0.144014</td>\n",
       "      <td>-0.132509</td>\n",
       "      <td>-0.061898</td>\n",
       "      <td>-0.129121</td>\n",
       "      <td>-0.177694</td>\n",
       "      <td>-0.218672</td>\n",
       "      <td>-0.140828</td>\n",
       "      <td>-0.113607</td>\n",
       "      <td>-0.104414</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167762</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.622177</td>\n",
       "      <td>0.114990</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.201232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28400_AAA_2013J</th>\n",
       "      <td>-0.144014</td>\n",
       "      <td>-0.132509</td>\n",
       "      <td>-0.061898</td>\n",
       "      <td>-0.129121</td>\n",
       "      <td>-0.177694</td>\n",
       "      <td>-0.218672</td>\n",
       "      <td>-0.140828</td>\n",
       "      <td>-0.113607</td>\n",
       "      <td>-0.104414</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551847</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.612431</td>\n",
       "      <td>0.170018</td>\n",
       "      <td>215.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.393053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32885_AAA_2013J</th>\n",
       "      <td>-0.144014</td>\n",
       "      <td>-0.132509</td>\n",
       "      <td>-0.061898</td>\n",
       "      <td>-0.129121</td>\n",
       "      <td>-0.177694</td>\n",
       "      <td>-0.218672</td>\n",
       "      <td>-0.140828</td>\n",
       "      <td>-0.113607</td>\n",
       "      <td>-0.104414</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>...</td>\n",
       "      <td>1.436596</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.713911</td>\n",
       "      <td>0.196850</td>\n",
       "      <td>295.0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.774278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 clicks_dataplus_w00  clicks_dataplus_w01  \\\n",
       "unique_id                                                   \n",
       "11391_AAA_2013J            -0.144014            -0.132509   \n",
       "28400_AAA_2013J            -0.144014            -0.132509   \n",
       "32885_AAA_2013J            -0.144014            -0.132509   \n",
       "\n",
       "                 clicks_dataplus_w02  clicks_dataplus_w03  \\\n",
       "unique_id                                                   \n",
       "11391_AAA_2013J            -0.061898            -0.129121   \n",
       "28400_AAA_2013J            -0.061898            -0.129121   \n",
       "32885_AAA_2013J            -0.061898            -0.129121   \n",
       "\n",
       "                 clicks_dataplus_w04  clicks_dataplus_w05  \\\n",
       "unique_id                                                   \n",
       "11391_AAA_2013J            -0.177694            -0.218672   \n",
       "28400_AAA_2013J            -0.177694            -0.218672   \n",
       "32885_AAA_2013J            -0.177694            -0.218672   \n",
       "\n",
       "                 clicks_dataplus_w06  clicks_dataplus_w07  \\\n",
       "unique_id                                                   \n",
       "11391_AAA_2013J            -0.140828            -0.113607   \n",
       "28400_AAA_2013J            -0.140828            -0.113607   \n",
       "32885_AAA_2013J            -0.140828            -0.113607   \n",
       "\n",
       "                 clicks_dataplus_w08  clicks_dataplus_w09  ...  \\\n",
       "unique_id                                                  ...   \n",
       "11391_AAA_2013J            -0.104414            -0.087706  ...   \n",
       "28400_AAA_2013J            -0.104414            -0.087706  ...   \n",
       "32885_AAA_2013J            -0.104414            -0.087706  ...   \n",
       "\n",
       "                 total_clicks_w11  active_weeks_uptoW  early_ratio_uptoW  \\\n",
       "unique_id                                                                  \n",
       "11391_AAA_2013J          1.167762                10.0           0.622177   \n",
       "28400_AAA_2013J          0.551847                11.0           0.612431   \n",
       "32885_AAA_2013J          1.436596                10.0           0.713911   \n",
       "\n",
       "                 late_ratio_uptoW  prestart_clicks_total  \\\n",
       "unique_id                                                  \n",
       "11391_AAA_2013J          0.114990                   98.0   \n",
       "28400_AAA_2013J          0.170018                  215.0   \n",
       "32885_AAA_2013J          0.196850                  295.0   \n",
       "\n",
       "                 prestart_active_days  prestart_active_weeks  \\\n",
       "unique_id                                                      \n",
       "11391_AAA_2013J                     1                      1   \n",
       "28400_AAA_2013J                     7                      2   \n",
       "32885_AAA_2013J                     8                      2   \n",
       "\n",
       "                 prestart_earliest_day  investigated_platform  \\\n",
       "unique_id                                                       \n",
       "11391_AAA_2013J                   -5.0                      1   \n",
       "28400_AAA_2013J                  -10.0                      1   \n",
       "32885_AAA_2013J                  -10.0                      1   \n",
       "\n",
       "                 prestart_ratio_vs_uptoW  \n",
       "unique_id                                 \n",
       "11391_AAA_2013J                 0.201232  \n",
       "28400_AAA_2013J                 0.393053  \n",
       "32885_AAA_2013J                 0.774278  \n",
       "\n",
       "[3 rows x 236 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA 5] Prestart + secuencia semanal + robust scaling (log1p + clip) + normalizaciÃ³n por curso (fit TRAIN)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EPS = 1e-6\n",
    "\n",
    "def _ensure_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0.0)\n",
    "    return out\n",
    "\n",
    "def add_prestart_features(\n",
    "    df_interactions: pd.DataFrame,\n",
    "    index_uids: pd.Index,\n",
    "    clicks_col: str = \"sum_click\",\n",
    "    day_col: str = \"date\",\n",
    ") -> pd.DataFrame:\n",
    "    df = df_interactions.copy()\n",
    "    df = add_unique_id(df)\n",
    "\n",
    "    df[clicks_col] = pd.to_numeric(df[clicks_col], errors=\"coerce\").fillna(0).astype(float)\n",
    "    df[day_col] = pd.to_numeric(df[day_col], errors=\"coerce\").astype(float)\n",
    "\n",
    "    pre = df[df[day_col] < 0].copy()\n",
    "\n",
    "    out = pd.DataFrame(index=index_uids)\n",
    "    if pre.empty:\n",
    "        out[\"prestart_clicks_total\"] = 0.0\n",
    "        out[\"prestart_active_days\"] = 0\n",
    "        out[\"prestart_active_weeks\"] = 0\n",
    "        out[\"prestart_earliest_day\"] = 0.0\n",
    "        out[\"investigated_platform\"] = 0\n",
    "        return out\n",
    "\n",
    "    pre[\"week\"] = np.floor(pre[day_col] / 7.0).astype(int)\n",
    "\n",
    "    agg = pre.groupby(\"unique_id\").agg(\n",
    "        prestart_clicks_total=(clicks_col, \"sum\"),\n",
    "        prestart_active_days=(day_col, lambda s: int(pd.Series(s.dropna().astype(int)).nunique())),\n",
    "        prestart_active_weeks=(\"week\", lambda s: int(pd.Series(s.dropna().astype(int)).nunique())),\n",
    "        prestart_earliest_day=(day_col, \"min\"),\n",
    "    )\n",
    "\n",
    "    agg = agg.reindex(index=index_uids, fill_value=0.0)\n",
    "    agg[\"prestart_active_days\"] = agg[\"prestart_active_days\"].astype(int)\n",
    "    agg[\"prestart_active_weeks\"] = agg[\"prestart_active_weeks\"].astype(int)\n",
    "    agg[\"prestart_clicks_total\"] = agg[\"prestart_clicks_total\"].astype(float)\n",
    "    agg[\"prestart_earliest_day\"] = agg[\"prestart_earliest_day\"].astype(float)\n",
    "\n",
    "    agg[\"investigated_platform\"] = (agg[\"prestart_clicks_total\"] > 0).astype(int)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def build_weekly_sequence_features(\n",
    "    df_interactions: pd.DataFrame,\n",
    "    index_uids: pd.Index,\n",
    "    weeks: int,\n",
    "    activity_col: str = \"activity_type\",\n",
    "    clicks_col: str = \"sum_click\",\n",
    "    day_col: str = \"date\",\n",
    ") -> pd.DataFrame:\n",
    "    df = df_interactions.copy()\n",
    "    df = add_unique_id(df)\n",
    "\n",
    "    df[clicks_col] = pd.to_numeric(df[clicks_col], errors=\"coerce\").fillna(0).astype(float)\n",
    "    df[day_col] = pd.to_numeric(df[day_col], errors=\"coerce\").astype(float)\n",
    "\n",
    "    df[\"week\"] = np.floor(df[day_col] / 7.0).astype(int)\n",
    "    df = df[(df[\"week\"] >= 0) & (df[\"week\"] < weeks)].copy()\n",
    "\n",
    "    g = df.groupby([\"unique_id\", \"week\", activity_col])[clicks_col].sum().reset_index()\n",
    "\n",
    "    pivot = g.pivot_table(\n",
    "        index=\"unique_id\",\n",
    "        columns=[\"week\", activity_col],\n",
    "        values=clicks_col,\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0.0,\n",
    "    )\n",
    "    pivot.columns = [f\"clicks_{str(act).lower()}_w{int(w):02d}\" for (w, act) in pivot.columns]\n",
    "    pivot = pivot.sort_index(axis=1)\n",
    "\n",
    "    weekly_total = df.groupby([\"unique_id\", \"week\"])[clicks_col].sum().unstack(fill_value=0.0)\n",
    "    for w in range(weeks):\n",
    "        if w not in weekly_total.columns:\n",
    "            weekly_total[w] = 0.0\n",
    "    weekly_total = weekly_total[sorted(weekly_total.columns)]\n",
    "    weekly_total.columns = [f\"total_clicks_w{int(w):02d}\" for w in weekly_total.columns]\n",
    "\n",
    "    active_weeks = (weekly_total.values > 0).sum(axis=1)\n",
    "    active_weeks = pd.Series(active_weeks, index=weekly_total.index, name=\"active_weeks_uptoW\")\n",
    "\n",
    "    total_sum = weekly_total.sum(axis=1).astype(float)\n",
    "    early_weeks = list(range(min(4, weeks)))\n",
    "    early_sum = weekly_total[[f\"total_clicks_w{w:02d}\" for w in early_weeks]].sum(axis=1).astype(float)\n",
    "    early_ratio = (early_sum / (total_sum + EPS)).fillna(0.0).rename(\"early_ratio_uptoW\")\n",
    "\n",
    "    late_weeks = list(range(max(0, weeks - 4), weeks))\n",
    "    late_sum = weekly_total[[f\"total_clicks_w{w:02d}\" for w in late_weeks]].sum(axis=1).astype(float)\n",
    "    late_ratio = (late_sum / (total_sum + EPS)).fillna(0.0).rename(\"late_ratio_uptoW\")\n",
    "\n",
    "    out = pivot.join(weekly_total, how=\"outer\").join(active_weeks).join(early_ratio).join(late_ratio).fillna(0.0)\n",
    "    out = out.reindex(index=index_uids, fill_value=0.0)\n",
    "    out.index.name = \"unique_id\"\n",
    "    return out\n",
    "\n",
    "\n",
    "def clip_and_log1p(df: pd.DataFrame, p_clip: float = 0.995) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) Clip por percentil (solo sobre columnas de clicks)\n",
    "    2) log1p\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    click_cols = [c for c in out.columns if c.startswith(\"clicks_\") or c.startswith(\"total_clicks_\")]\n",
    "    if not click_cols:\n",
    "        return out\n",
    "\n",
    "    # clip global (mÃ¡s simple y robusto); evita outliers 500000\n",
    "    vals = out[click_cols].values.astype(float).ravel()\n",
    "    clip_val = float(np.quantile(vals, p_clip))\n",
    "    if clip_val <= 0:\n",
    "        clip_val = 0.0\n",
    "\n",
    "    out[click_cols] = np.clip(out[click_cols].astype(float), 0.0, clip_val)\n",
    "    out[click_cols] = np.log1p(out[click_cols].astype(float))\n",
    "    return out\n",
    "\n",
    "def fit_course_stats(\n",
    "    X_seq: pd.DataFrame,\n",
    "    course_series: pd.Series,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Fit medias/std por curso SOLO para columnas de clicks.\n",
    "    \"\"\"\n",
    "    X_seq = X_seq.copy()\n",
    "    course_series = course_series.reindex(X_seq.index)\n",
    "    course_series = course_series.fillna(\"UNKNOWN\").astype(str)\n",
    "\n",
    "    click_cols = [c for c in X_seq.columns if c.startswith(\"clicks_\") or c.startswith(\"total_clicks_\")]\n",
    "    stats = {}\n",
    "\n",
    "    for course, idxs in course_series.groupby(course_series).groups.items():\n",
    "        block = X_seq.loc[idxs, click_cols]\n",
    "        mu = block.mean(axis=0)\n",
    "        sd = block.std(axis=0).replace(0, 1.0)\n",
    "        stats[str(course)] = {c: {\"mean\": float(mu[c]), \"std\": float(sd[c])} for c in click_cols}\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def apply_course_norm(\n",
    "    X_seq: pd.DataFrame,\n",
    "    course_series: pd.Series,\n",
    "    course_stats: dict,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica z-score por curso SOLO a columnas de clicks.\n",
    "    Ratios (0..1) y features prestart no se normalizan por curso.\n",
    "    \"\"\"\n",
    "    out = X_seq.copy()\n",
    "    course_series = course_series.reindex(out.index)\n",
    "    course_series = course_series.fillna(\"UNKNOWN\").astype(str)\n",
    "\n",
    "    click_cols = [c for c in out.columns if c.startswith(\"clicks_\") or c.startswith(\"total_clicks_\")]\n",
    "    if not click_cols:\n",
    "        return out\n",
    "\n",
    "    # fallback global\n",
    "    global_mu = out[click_cols].mean(axis=0)\n",
    "    global_sd = out[click_cols].std(axis=0).replace(0, 1.0)\n",
    "\n",
    "    for course, idxs in course_series.groupby(course_series).groups.items():\n",
    "        ckey = str(course)\n",
    "        if ckey in course_stats:\n",
    "            mu = pd.Series({col: course_stats[ckey][col][\"mean\"] for col in click_cols})\n",
    "            sd = pd.Series({col: course_stats[ckey][col][\"std\"] for col in click_cols}).replace(0, 1.0)\n",
    "        else:\n",
    "            mu, sd = global_mu, global_sd\n",
    "\n",
    "        out.loc[idxs, click_cols] = (out.loc[idxs, click_cols] - mu) / sd\n",
    "\n",
    "    out[click_cols] = out[click_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_transformer_block_for_split(\n",
    "    split_students_al: pd.DataFrame,     # index=unique_id, con code_module/code_presentation\n",
    "    split_interactions: pd.DataFrame,    # raw interactions del split\n",
    "    weeks: int,\n",
    "    course_stats: dict | None = None,    # si None => fit en este split (solo TRAIN)\n",
    "    p_clip: float = 0.995,\n",
    "):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - X_seq_norm: secuencia semanal (0..W-1) + features agregadas + prestart, ya normalizado\n",
    "      - course_stats (si se fittea en este split)\n",
    "    \"\"\"\n",
    "    # course key (module_presentation)\n",
    "    course = (\n",
    "        split_students_al[\"code_module\"].astype(str)\n",
    "        + \"_\"\n",
    "        + split_students_al[\"code_presentation\"].astype(str)\n",
    "    )\n",
    "\n",
    "    # 1) secuencia + agregados\n",
    "    X_seq = build_weekly_sequence_features(split_interactions, split_students_al.index, weeks=weeks)\n",
    "\n",
    "    # 2) prestart\n",
    "    X_pre = add_prestart_features(split_interactions, split_students_al.index)\n",
    "\n",
    "    # ratio prestart vs actividad en ventana (sobre TOTAL semanal)\n",
    "    total_uptoW = X_seq[[c for c in X_seq.columns if c.startswith(\"total_clicks_w\")]].sum(axis=1).astype(float)\n",
    "    X_pre[\"prestart_ratio_vs_uptoW\"] = (X_pre[\"prestart_clicks_total\"] / (total_uptoW + EPS)).fillna(0.0)\n",
    "\n",
    "    # 3) unir\n",
    "    X = pd.concat([X_seq, X_pre], axis=1)\n",
    "\n",
    "    # 4) robust anti-outliers en clicks (clip + log1p)\n",
    "    X = clip_and_log1p(X, p_clip=p_clip)\n",
    "\n",
    "    # 5) normalizaciÃ³n por curso (fit solo en TRAIN)\n",
    "    if course_stats is None:\n",
    "        course_stats = fit_course_stats(X, course)\n",
    "    X = apply_course_norm(X, course, course_stats)\n",
    "\n",
    "    return X, course_stats\n",
    "\n",
    "\n",
    "# --------- EJEMPLO (TRAIN, W=12) ---------\n",
    "W = 12\n",
    "X_train_w12, course_stats_train = build_transformer_block_for_split(\n",
    "    split_students_al=train_students_al,\n",
    "    split_interactions=train_interactions,\n",
    "    weeks=W,\n",
    "    course_stats=None,      # FIT aquÃ­ (solo TRAIN)\n",
    "    p_clip=0.995,\n",
    ")\n",
    "\n",
    "print(\"âœ… X_train_w12 (seq+prestart+norm):\", X_train_w12.shape)\n",
    "display(X_train_w12.head(3))\n",
    "\n",
    "# Guardamos stats en memoria para aplicar en val/test luego\n",
    "COURSE_STATS = course_stats_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b7f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_students_al' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# [CELDA 7] Generar y Guardar VALIDATION y TEST (Usando stats de Train)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# IMPORTANTE: Usamos 'COURSE_STATS' calculado en la celda anterior (Train)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# para que la normalizaciÃ³n sea consistente y sin data leakage.\u001b[39;00m\n\u001b[32m      7\u001b[39m splits_to_process = [\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mval_students_al\u001b[49m, val_interactions_al, val_target_al),\n\u001b[32m      9\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, test_students_al, test_interactions_al, test_target_al)\n\u001b[32m     10\u001b[39m ]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split_name, df_stud, df_inter, df_tgt \u001b[38;5;129;01min\u001b[39;00m splits_to_process:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸš€ Procesando split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (W=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_students_al' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA 7] Generar y Guardar VALIDATION y TEST (Usando stats de Train)\n",
    "# =========================\n",
    "# IMPORTANTE: Usamos 'COURSE_STATS' calculado en la celda anterior (Train)\n",
    "# para que la normalizaciÃ³n sea consistente y sin data leakage.\n",
    "\n",
    "splits_to_process = [\n",
    "    (\"validation\", val_students_al, val_interactions_al, val_target_al),\n",
    "    (\"test\", test_students_al, test_interactions_al, test_target_al)\n",
    "]\n",
    "\n",
    "for split_name, df_stud, df_inter, df_tgt in splits_to_process:\n",
    "    print(f\"\\nðŸš€ Procesando split: {split_name.upper()} (W={W}) ...\")\n",
    "    \n",
    "    # Notar que pasamos course_stats=COURSE_STATS (el de train)\n",
    "    X_split, _ = build_transformer_block_for_split(\n",
    "        split_students_al=df_stud,\n",
    "        split_interactions=df_inter,\n",
    "        weeks=W,\n",
    "        course_stats=COURSE_STATS,  # <--- CLAVE: Usar stats de train\n",
    "        p_clip=0.995,\n",
    "    )\n",
    "    \n",
    "    # Alinear target\n",
    "    y_split = df_tgt.loc[X_split.index].copy()\n",
    "    \n",
    "    # Checks bÃ¡sicos\n",
    "    print(f\"   -> Shape X: {X_split.shape}, y: {y_split.shape}\")\n",
    "    if X_split.isna().sum().sum() > 0:\n",
    "        print(f\"   âš ï¸ ALERTA: NaNs detectados en {split_name}!\")\n",
    "        \n",
    "    # Guardado\n",
    "    out_dir = OUT_DIR / split_name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    X_split.to_csv(out_dir / f\"w{W:02d}_X_seq.csv\")\n",
    "    y_split.to_csv(out_dir / f\"w{W:02d}_y.csv\")\n",
    "    \n",
    "    print(f\"   ðŸ’¾ Guardado en: {out_dir}\")\n",
    "\n",
    "print(\"\\nâœ… Todos los splits generados para ventana W =\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7df56",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No existe FE2: /workspace/TFM_education_ai_analytics/data/6_transformer_features/validation/w12_X_seq.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# ---- LOAD DATA ----\u001b[39;00m\n\u001b[32m     79\u001b[39m students = {split: load_students(split) \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m SPLITS}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m X_seq = \u001b[43m{\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_fe2_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSPLITS\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… loaded students:\u001b[39m\u001b[33m\"\u001b[39m, {k: v.shape \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m students.items()})\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… loaded FE2 seq :\u001b[39m\u001b[33m\"\u001b[39m, {k: v.shape \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m X_seq.items()})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# ---- LOAD DATA ----\u001b[39;00m\n\u001b[32m     79\u001b[39m students = {split: load_students(split) \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m SPLITS}\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m X_seq = {split: \u001b[43mload_fe2_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m SPLITS}\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… loaded students:\u001b[39m\u001b[33m\"\u001b[39m, {k: v.shape \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m students.items()})\n\u001b[32m     83\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… loaded FE2 seq :\u001b[39m\u001b[33m\"\u001b[39m, {k: v.shape \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m X_seq.items()})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mload_fe2_seq\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     42\u001b[39m path = os.path.join(FE2_DIR, split, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_X_seq.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(path):\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo existe FE2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(path, index_col=\u001b[32m0\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No existe FE2: /workspace/TFM_education_ai_analytics/data/6_transformer_features/validation/w12_X_seq.csv"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [CELDA INDEPENDIENTE - AUTÃ“NOMA] EstÃ¡ticas desde students.csv + schema TRAIN + scaler + merge con FE2 + guardado\n",
    "# =========================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ---- CONFIG ----\n",
    "W = 12  # cambia a 16/20/24 si quieres\n",
    "\n",
    "# Ruta raÃ­z del repo (ajÃºstala si tu notebook no estÃ¡ en /workspace/TFM_education_ai_analytics)\n",
    "PROJECT_DIR = \"/workspace/TFM_education_ai_analytics\"\n",
    "\n",
    "PROCESSED_DIR = os.path.join(PROJECT_DIR, \"data/2_processed\")\n",
    "FE2_DIR = os.path.join(PROJECT_DIR, \"data/6_transformer_features\")\n",
    "\n",
    "SPLITS = [\"training\", \"validation\", \"test\"]\n",
    "\n",
    "STATIC_CATEGORICAL = [\"gender\", \"region\", \"highest_education\", \"imd_band\", \"age_band\", \"disability\"]\n",
    "STATIC_NUMERIC = [\"num_of_prev_attempts\", \"studied_credits\", \"date_registration\", \"module_presentation_length\"]\n",
    "EXCLUDE_COLS = [\"final_result\", \"date_unregistration\"]  # leakage / target-like\n",
    "\n",
    "# ---- HELPERS ----\n",
    "def build_unique_id(df_students: pd.DataFrame) -> pd.Series:\n",
    "    return (\n",
    "        df_students[\"id_student\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_students[\"code_module\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_students[\"code_presentation\"].astype(str)\n",
    "    )\n",
    "\n",
    "def load_students(split: str) -> pd.DataFrame:\n",
    "    path = os.path.join(PROCESSED_DIR, split, \"students.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No existe: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def load_fe2_seq(split: str) -> pd.DataFrame:\n",
    "    path = os.path.join(FE2_DIR, split, f\"w{W:02d}_X_seq.csv\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"No existe FE2: {path}\")\n",
    "    return pd.read_csv(path, index_col=0)\n",
    "\n",
    "def build_static_from_students(df_students: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_students.copy()\n",
    "\n",
    "    # Excluir columnas peligrosas\n",
    "    for c in EXCLUDE_COLS:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    # unique_id\n",
    "    df[\"unique_id\"] = build_unique_id(df)\n",
    "    df = df.set_index(\"unique_id\")\n",
    "\n",
    "    # NumÃ©ricas\n",
    "    num_cols = [c for c in STATIC_NUMERIC if c in df.columns]\n",
    "    for c in num_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "\n",
    "    # CategÃ³ricas â†’ OHE\n",
    "    cat_cols = [c for c in STATIC_CATEGORICAL if c in df.columns]\n",
    "    X_cat = pd.get_dummies(df[cat_cols].fillna(\"UNKNOWN\").astype(str), prefix=cat_cols)\n",
    "\n",
    "    X_num = df[num_cols].copy()\n",
    "    X = pd.concat([X_num, X_cat], axis=1)\n",
    "\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "    X.index.name = \"unique_id\"\n",
    "    return X\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# ---- LOAD DATA ----\n",
    "students = {split: load_students(split) for split in SPLITS}\n",
    "X_seq = {split: load_fe2_seq(split) for split in SPLITS}\n",
    "\n",
    "print(\"âœ… loaded students:\", {k: v.shape for k, v in students.items()})\n",
    "print(\"âœ… loaded FE2 seq :\", {k: v.shape for k, v in X_seq.items()})\n",
    "\n",
    "# ---- BUILD STATIC (TRAIN defines schema + scaler) ----\n",
    "X_static_train = build_static_from_students(students[\"training\"])\n",
    "static_schema = X_static_train.columns.tolist()\n",
    "\n",
    "num_cols_in_schema = [c for c in STATIC_NUMERIC if c in X_static_train.columns]\n",
    "\n",
    "scaler_static = StandardScaler()\n",
    "if num_cols_in_schema:\n",
    "    X_static_train[num_cols_in_schema] = scaler_static.fit_transform(X_static_train[num_cols_in_schema])\n",
    "\n",
    "def build_static_split(split: str) -> pd.DataFrame:\n",
    "    Xs = build_static_from_students(students[split])\n",
    "    Xs = Xs.reindex(columns=static_schema, fill_value=0.0)\n",
    "    cols = [c for c in num_cols_in_schema if c in Xs.columns]\n",
    "    if cols:\n",
    "        Xs[cols] = scaler_static.transform(Xs[cols])\n",
    "    return Xs.replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "X_static = {\n",
    "    \"training\": X_static_train,\n",
    "    \"validation\": build_static_split(\"validation\"),\n",
    "    \"test\": build_static_split(\"test\"),\n",
    "}\n",
    "\n",
    "print(\"âœ… static shapes:\", {k: v.shape for k, v in X_static.items()})\n",
    "\n",
    "# ---- MERGE STATIC + FE2 ----\n",
    "X_hybrid = {}\n",
    "for split in SPLITS:\n",
    "    X_hybrid[split] = X_seq[split].join(X_static[split], how=\"left\").fillna(0.0)\n",
    "    print(f\"âœ… hybrid {split}: {X_hybrid[split].shape}\")\n",
    "\n",
    "# ---- SAVE ----\n",
    "for split in SPLITS:\n",
    "    out_dir = os.path.join(FE2_DIR, split)\n",
    "    ensure_dir(out_dir)\n",
    "    out_path = os.path.join(out_dir, f\"w{W:02d}_X_hybrid.csv\")\n",
    "    X_hybrid[split].to_csv(out_path)\n",
    "    print(\"ðŸ’¾ guardado:\", out_path)\n",
    "\n",
    "# Preview\n",
    "display(X_hybrid[\"training\"].head(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
