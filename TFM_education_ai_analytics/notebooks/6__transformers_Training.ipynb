{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62802b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_WINDOWS: [12, 18, 24]\n",
      "base: (22785, 16)\n",
      "seg W12: (22785, 11)\n",
      "seg W18: (22785, 11)\n",
      "seg W24: (22785, 11)\n",
      "all: (22785, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imd_band</th>\n",
       "      <th>age_band</th>\n",
       "      <th>highest_education</th>\n",
       "      <th>num_of_prev_attempts</th>\n",
       "      <th>studied_credits</th>\n",
       "      <th>region_encoded</th>\n",
       "      <th>prestart_clicks_total</th>\n",
       "      <th>prestart_active_days</th>\n",
       "      <th>prestart_active_weeks</th>\n",
       "      <th>prestart_earliest_day</th>\n",
       "      <th>...</th>\n",
       "      <th>w24_cluster_label</th>\n",
       "      <th>w24_cluster_name</th>\n",
       "      <th>w24_p_cluster_0</th>\n",
       "      <th>w24_p_cluster_1</th>\n",
       "      <th>w24_p_cluster_2</th>\n",
       "      <th>w24_p_cluster_3</th>\n",
       "      <th>w24_p_cluster_4</th>\n",
       "      <th>w24_confidence</th>\n",
       "      <th>w24_entropy</th>\n",
       "      <th>w24_entropy_norm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11391_AAA_2013J</th>\n",
       "      <td>1.356077</td>\n",
       "      <td>2.315221</td>\n",
       "      <td>1.230868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.963451</td>\n",
       "      <td>-1.582024</td>\n",
       "      <td>0.329978</td>\n",
       "      <td>-1.123904</td>\n",
       "      <td>-0.914229</td>\n",
       "      <td>0.960497</td>\n",
       "      <td>...</td>\n",
       "      <td>CONSISTENT_GOOD</td>\n",
       "      <td>Consistentes (buen nivel)</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.814712e-15</td>\n",
       "      <td>9.997444e-01</td>\n",
       "      <td>1.113072e-15</td>\n",
       "      <td>0.999744</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.001470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28400_AAA_2013J</th>\n",
       "      <td>-1.446584</td>\n",
       "      <td>0.682052</td>\n",
       "      <td>1.230868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.526720</td>\n",
       "      <td>-0.011035</td>\n",
       "      <td>0.766746</td>\n",
       "      <td>0.937025</td>\n",
       "      <td>0.605604</td>\n",
       "      <td>-0.569965</td>\n",
       "      <td>...</td>\n",
       "      <td>METHODICAL_EXPLORER</td>\n",
       "      <td>Exploradores metódicos</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.437378e-04</td>\n",
       "      <td>8.682577e-10</td>\n",
       "      <td>9.131973e-20</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.003339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32885_AAA_2013J</th>\n",
       "      <td>-0.245444</td>\n",
       "      <td>-0.951118</td>\n",
       "      <td>-1.413218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.526720</td>\n",
       "      <td>1.298122</td>\n",
       "      <td>0.943143</td>\n",
       "      <td>1.280514</td>\n",
       "      <td>0.605604</td>\n",
       "      <td>-0.569965</td>\n",
       "      <td>...</td>\n",
       "      <td>METHODICAL_EXPLORER</td>\n",
       "      <td>Exploradores metódicos</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.651113e-17</td>\n",
       "      <td>3.850084e-15</td>\n",
       "      <td>1.619867e-06</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 imd_band  age_band  highest_education  num_of_prev_attempts  \\\n",
       "unique_id                                                                      \n",
       "11391_AAA_2013J  1.356077  2.315221           1.230868                   0.0   \n",
       "28400_AAA_2013J -1.446584  0.682052           1.230868                   0.0   \n",
       "32885_AAA_2013J -0.245444 -0.951118          -1.413218                   0.0   \n",
       "\n",
       "                 studied_credits  region_encoded  prestart_clicks_total  \\\n",
       "unique_id                                                                 \n",
       "11391_AAA_2013J         2.963451       -1.582024               0.329978   \n",
       "28400_AAA_2013J        -0.526720       -0.011035               0.766746   \n",
       "32885_AAA_2013J        -0.526720        1.298122               0.943143   \n",
       "\n",
       "                 prestart_active_days  prestart_active_weeks  \\\n",
       "unique_id                                                      \n",
       "11391_AAA_2013J             -1.123904              -0.914229   \n",
       "28400_AAA_2013J              0.937025               0.605604   \n",
       "32885_AAA_2013J              1.280514               0.605604   \n",
       "\n",
       "                 prestart_earliest_day  ...    w24_cluster_label  \\\n",
       "unique_id                               ...                        \n",
       "11391_AAA_2013J               0.960497  ...      CONSISTENT_GOOD   \n",
       "28400_AAA_2013J              -0.569965  ...  METHODICAL_EXPLORER   \n",
       "32885_AAA_2013J              -0.569965  ...  METHODICAL_EXPLORER   \n",
       "\n",
       "                          w24_cluster_name  w24_p_cluster_0  w24_p_cluster_1  \\\n",
       "unique_id                                                                      \n",
       "11391_AAA_2013J  Consistentes (buen nivel)         0.000255              0.0   \n",
       "28400_AAA_2013J     Exploradores metódicos         0.999356              0.0   \n",
       "32885_AAA_2013J     Exploradores metódicos         0.999998              0.0   \n",
       "\n",
       "                 w24_p_cluster_2  w24_p_cluster_3  w24_p_cluster_4  \\\n",
       "unique_id                                                            \n",
       "11391_AAA_2013J     5.814712e-15     9.997444e-01     1.113072e-15   \n",
       "28400_AAA_2013J     6.437378e-04     8.682577e-10     9.131973e-20   \n",
       "32885_AAA_2013J     5.651113e-17     3.850084e-15     1.619867e-06   \n",
       "\n",
       "                w24_confidence w24_entropy  w24_entropy_norm  \n",
       "unique_id                                                     \n",
       "11391_AAA_2013J       0.999744    0.002366          0.001470  \n",
       "28400_AAA_2013J       0.999356    0.005375          0.003339  \n",
       "32885_AAA_2013J       0.999998    0.000023          0.000015  \n",
       "\n",
       "[3 rows x 49 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga target + day0 + segmentaciones de training por semana (W_WINDOWS desde config.py)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from educational_ai_analytics.config import W_WINDOWS  # array de semanas\n",
    "\n",
    "BASE_FEAT = Path(\"/workspace/TFM_education_ai_analytics/data/3_features/training\")\n",
    "BASE_SEG = Path(\"/workspace/TFM_education_ai_analytics/data/5_students_segmented/training\")\n",
    "\n",
    "# 1) Target + Day0\n",
    "target = pd.read_csv(BASE_FEAT / \"target.csv\", index_col=0)\n",
    "day0 = pd.read_csv(BASE_FEAT / \"day0_static_features.csv\", index_col=0)\n",
    "\n",
    "# 2) Segmentaciones por semana\n",
    "seg_by_w = {}\n",
    "for w in sorted(W_WINDOWS):\n",
    "    p = BASE_SEG / f\"students_segmented_uptW{int(w)}.csv\"\n",
    "    seg = pd.read_csv(p, index_col=0)\n",
    "    seg = seg.add_prefix(f\"w{int(w)}_\")\n",
    "    seg_by_w[int(w)] = seg\n",
    "\n",
    "# 3) Dataset base alineado\n",
    "common_idx = target.index.intersection(day0.index)\n",
    "df_base = day0.loc[common_idx].join(target.loc[common_idx], how=\"inner\")\n",
    "\n",
    "# 4) DataFrame combinado con todas las segmentaciones\n",
    "df_all = df_base.copy()\n",
    "for w, seg in seg_by_w.items():\n",
    "    df_all = df_all.join(seg, how=\"left\")\n",
    "\n",
    "print(\"W_WINDOWS:\", W_WINDOWS)\n",
    "print(\"base:\", df_base.shape)\n",
    "for w in sorted(seg_by_w):\n",
    "    print(f\"seg W{w}:\", seg_by_w[w].shape)\n",
    "print(\"all:\", df_all.shape)\n",
    "\n",
    "df_all.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75b03d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== WEEK 12 ========\n",
      "X_seq: (19816, 12, 20)\n",
      "mask : (19816, 12)\n",
      "y    : (19816,)\n",
      "n_activities: 20\n",
      "classes: (array([0, 1, 2, 3]), array([4532, 4576, 8599, 2109]))\n",
      "================================\n",
      "======== WEEK 18 ========\n",
      "X_seq: (19859, 18, 20)\n",
      "mask : (19859, 18)\n",
      "y    : (19859,)\n",
      "n_activities: 20\n",
      "classes: (array([0, 1, 2, 3]), array([4541, 4596, 8611, 2111]))\n",
      "================================\n",
      "======== WEEK 24 ========\n",
      "X_seq: (19874, 24, 20)\n",
      "mask : (19874, 24)\n",
      "y    : (19874,)\n",
      "n_activities: 20\n",
      "classes: (array([0, 1, 2, 3]), array([4545, 4605, 8613, 2111]))\n",
      "================================\n",
      "\n",
      "W_KEY: upto_12 | X: (19816, 12, 20) | M: (19816, 12) | y: (19816,) | ids: 19816\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from educational_ai_analytics.config import W_WINDOWS\n",
    "\n",
    "BASE_PROCESSED = Path(\"/workspace/TFM_education_ai_analytics/data/2_processed/training\")\n",
    "BASE_TARGET = Path(\"/workspace/TFM_education_ai_analytics/data/3_features/training/target.csv\")\n",
    "\n",
    "# Carga\n",
    "interactions_df = pd.read_csv(BASE_PROCESSED / \"interactions.csv\", index_col=None)\n",
    "target_full = pd.read_csv(BASE_TARGET, index_col=0).sort_index()\n",
    "\n",
    "# Robustez columna clicks\n",
    "click_col = \"sum_click\" if \"sum_click\" in interactions_df.columns else \"clicks\"\n",
    "if click_col not in interactions_df.columns:\n",
    "    raise ValueError(\"No se encontró ni 'sum_click' ni 'clicks' en interactions.csv\")\n",
    "\n",
    "# Limpieza y unique_id\n",
    "interactions_df[\"activity_type\"] = interactions_df[\"activity_type\"].astype(str).str.strip().str.lower()\n",
    "interactions_df[\"unique_id\"] = (\n",
    "    interactions_df[\"id_student\"].astype(str)\n",
    "    .str.cat(interactions_df[\"code_module\"].astype(str), sep=\"_\")\n",
    "    .str.cat(interactions_df[\"code_presentation\"].astype(str), sep=\"_\")\n",
    ")\n",
    "\n",
    "# Semana 0-based\n",
    "interactions_df[\"week\"] = (\n",
    "    pd.to_numeric(interactions_df[\"date\"], errors=\"coerce\").fillna(-9999) // 7\n",
    ").astype(int)\n",
    "\n",
    "# Base temporal válida\n",
    "interactions_base = interactions_df[interactions_df[\"week\"] >= 0].copy()\n",
    "\n",
    "# Actividades globales (mismo F para todas las ventanas)\n",
    "activities_global = sorted(interactions_base[\"activity_type\"].unique().tolist())\n",
    "\n",
    "sequences = {f\"upto_{w}\": None for w in sorted(W_WINDOWS)}\n",
    "masks = {f\"upto_{w}\": None for w in sorted(W_WINDOWS)}\n",
    "labels = {f\"upto_{w}\": None for w in sorted(W_WINDOWS)}\n",
    "ids = {f\"upto_{w}\": None for w in sorted(W_WINDOWS)}\n",
    "\n",
    "for upto_week in sorted(W_WINDOWS):\n",
    "    inter_uptoW = interactions_base[interactions_base[\"week\"] < upto_week].copy()\n",
    "\n",
    "    g = (\n",
    "        inter_uptoW.groupby([\"unique_id\", \"week\", \"activity_type\"], as_index=False)[click_col]\n",
    "        .sum()\n",
    "        .rename(columns={click_col: \"sum_click\"})\n",
    "    )\n",
    "\n",
    "    weeks = list(range(upto_week))\n",
    "    full_cols = pd.MultiIndex.from_product(\n",
    "        [weeks, activities_global], names=[\"week\", \"activity_type\"]\n",
    "    )\n",
    "\n",
    "    wide = (\n",
    "        g.pivot_table(\n",
    "            index=\"unique_id\",\n",
    "            columns=[\"week\", \"activity_type\"],\n",
    "            values=\"sum_click\",\n",
    "            aggfunc=\"sum\",\n",
    "            fill_value=0,\n",
    "        )\n",
    "        .reindex(columns=full_cols, fill_value=0)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    # Orden estable basado en target (mejor para trazabilidad y evaluación)\n",
    "    common_ids = target_full.index.intersection(wide.index)\n",
    "    wide_w = wide.loc[common_ids]\n",
    "    target_w = target_full.loc[common_ids]\n",
    "\n",
    "    X_seq = wide_w.values.reshape(len(wide_w), upto_week, len(activities_global)).astype(np.float32)\n",
    "    mask = (X_seq.sum(axis=2) > 0).astype(np.int32)\n",
    "    y = target_w[\"final_result\"].astype(np.int64).values\n",
    "\n",
    "    key = f\"upto_{upto_week}\"\n",
    "    sequences[key] = X_seq\n",
    "    masks[key] = mask\n",
    "    labels[key] = y\n",
    "    ids[key] = common_ids\n",
    "\n",
    "    print(f\"======== WEEK {upto_week} ========\")\n",
    "    print(\"X_seq:\", X_seq.shape)\n",
    "    print(\"mask :\", mask.shape)\n",
    "    print(\"y    :\", y.shape)\n",
    "    print(\"n_activities:\", len(activities_global))\n",
    "    print(\"classes:\", np.unique(y, return_counts=True))\n",
    "    print(\"================================\")\n",
    "\n",
    "# Ejemplo de consumo consistente:\n",
    "W_KEY = f\"upto_{sorted(W_WINDOWS)[0]}\"\n",
    "X = sequences[W_KEY]\n",
    "M = masks[W_KEY]\n",
    "y = labels[W_KEY]\n",
    "uid = ids[W_KEY]\n",
    "\n",
    "print(\"\\nW_KEY:\", W_KEY, \"| X:\", X.shape, \"| M:\", M.shape, \"| y:\", y.shape, \"| ids:\", len(uid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73a3d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        pos = tf.range(max_len, dtype=tf.float32)[:, tf.newaxis]   # (L, 1)\n",
    "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]     # (1, D)\n",
    "\n",
    "        angle_rates = 1.0 / tf.pow(10000.0, (2.0 * tf.floor(i / 2.0)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "\n",
    "        sin_part = tf.sin(angle_rads[:, 0::2])\n",
    "        cos_part = tf.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pe = tf.concat([sin_part, cos_part], axis=-1)              # (L, D)\n",
    "        self.pe = pe[tf.newaxis, ...]                              # (1, L, D)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, d_model: int, num_heads: int, ff_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.mha = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model // num_heads,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.dropout1 = layers.Dropout(dropout)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add1 = layers.Add()\n",
    "\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(d_model),\n",
    "            ]\n",
    "        )\n",
    "        self.dropout2 = layers.Dropout(dropout)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.add2 = layers.Add()\n",
    "\n",
    "    def call(self, x, training=False, attention_mask=None):\n",
    "        attn_out = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        x = self.add1([x, attn_out])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        ffn_out = self.ffn(x, training=training)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        x = self.add2([x, ffn_out])\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLULayer(layers.Layer):\n",
    "    # h(X) = (XW) * sigmoid(XV)\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.proj = layers.Dense(2 * d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        z = self.proj(x)\n",
    "        a, b = tf.split(z, num_or_size_splits=2, axis=-1)\n",
    "        return a * tf.sigmoid(b)\n",
    "\n",
    "\n",
    "class GLUTransformerClassifier(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_d: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        dropout: float,\n",
    "        num_classes: int,\n",
    "        num_layers: int = 2,\n",
    "        max_len: int = 512,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if latent_d % num_heads != 0:\n",
    "            raise ValueError(\"latent_d debe ser divisible por num_heads\")\n",
    "\n",
    "        self.input_proj = layers.Dense(latent_d)\n",
    "        self.pos_encoding = PositionalEncoding(d_model=latent_d, max_len=max_len)\n",
    "        self.input_dropout = layers.Dropout(dropout)\n",
    "\n",
    "        self.encoders = [\n",
    "            TransformerEncoderBlock(\n",
    "                d_model=latent_d,\n",
    "                num_heads=num_heads,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.glu = GLULayer(d_model=latent_d)\n",
    "        self.norm_out = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.head = tf.keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_d, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "                layers.Dense(num_classes, activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        # Soporta model.fit(x=(X_seq, seq_mask), ...)\n",
    "        if isinstance(inputs, (tuple, list)):\n",
    "            x, seq_mask = inputs\n",
    "        else:\n",
    "            x = inputs\n",
    "            seq_mask = None\n",
    "\n",
    "        x = self.input_proj(x)                  # (B, W, D)\n",
    "        x = self.pos_encoding(x)                # (B, W, D)\n",
    "        x = self.input_dropout(x, training=training)\n",
    "\n",
    "        # Mask para atención: (B, 1, W) -> broadcast interno a (B, W, W)\n",
    "        attn_mask = None\n",
    "        if seq_mask is not None:\n",
    "            attn_mask = tf.cast(seq_mask[:, tf.newaxis, :], tf.bool)\n",
    "\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, training=training, attention_mask=attn_mask)\n",
    "\n",
    "        x = self.glu(x)\n",
    "        x = self.norm_out(x)\n",
    "\n",
    "        # Masked pooling temporal\n",
    "        if seq_mask is not None:\n",
    "            m = tf.cast(seq_mask, x.dtype)[:, :, tf.newaxis]  # (B, W, 1)\n",
    "            pooled = tf.reduce_sum(x * m, axis=1) / (tf.reduce_sum(m, axis=1) + 1e-8)\n",
    "        else:\n",
    "            pooled = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "        return self.head(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c796c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: {0: 1.0932413793103448, 1: 1.0824911226440863, 2: 0.5761011774967292, 3: 2.3491404860699467}\n",
      "Epoch 1/40\n",
      "\u001b[1m121/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3290 - loss: 1.3427"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ---------- Selección de ventana ----------\n",
    "W_KEY = \"upto_12\"  # cambia a: upto_18 / upto_24\n",
    "X = sequences[W_KEY].astype(np.float32)   # (N, W, F)\n",
    "M = masks[W_KEY].astype(np.int32)         # (N, W)\n",
    "y = labels[W_KEY].astype(np.int64)        # (N,)\n",
    "\n",
    "# ---------- Split train/val ----------\n",
    "X_tr, X_va, M_tr, M_va, y_tr, y_va = train_test_split(\n",
    "    X, M, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# ---------- Class weights (multiclase) ----------\n",
    "classes = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ---------- Modelo ----------\n",
    "model = GLUTransformerClassifier(\n",
    "    latent_d=128,      # paper-inspired\n",
    "    num_heads=4,       # 512 % 8 == 0\n",
    "    ff_dim=512,\n",
    "    dropout=0.1,\n",
    "    num_classes=4,     # multiclase\n",
    "    num_layers=2,\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=6,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "# ---------- Entrenamiento ----------\n",
    "history = model.fit(\n",
    "    x=(X_tr, M_tr),                  # (X_seq, seq_mask)\n",
    "    y=y_tr,\n",
    "    validation_data=((X_va, M_va), y_va),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- Evaluación rápida ----------\n",
    "val_loss, val_acc = model.evaluate((X_va, M_va), y_va, verbose=0)\n",
    "print(f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "print(\"best_val_acc:\", max(history.history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cee018eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_acc_last: 0.4547060430049896\n",
      "val_acc_last  : 0.425580233335495\n",
      "val_acc_best  : 0.425580233335495\n",
      "val_loss_eval : 1.1786150932312012\n",
      "val_acc_eval  : 0.42078709602355957\n"
     ]
    }
   ],
   "source": [
    "# Accuracy final de train/val (última época registrada)\n",
    "print(\"train_acc_last:\", history.history[\"accuracy\"][-1])\n",
    "print(\"val_acc_last  :\", history.history[\"val_accuracy\"][-1])\n",
    "\n",
    "# Mejor val_accuracy alcanzada\n",
    "print(\"val_acc_best  :\", max(history.history[\"val_accuracy\"]))\n",
    "\n",
    "# Evaluación explícita en validación\n",
    "val_loss, val_acc = model.evaluate((X_va, M_va), y_va, verbose=0)\n",
    "print(\"val_loss_eval :\", val_loss)\n",
    "print(\"val_acc_eval  :\", val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5516e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight: {0: 1.0932413793103448, 1: 1.0824911226440863, 2: 0.5761011774967292, 3: 2.3491404860699467}\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     39\u001b[39m model.compile(\n\u001b[32m     40\u001b[39m     optimizer=optimizer,\n\u001b[32m     41\u001b[39m     loss=\u001b[33m\"\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     42\u001b[39m     metrics=[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     43\u001b[39m )\n\u001b[32m     45\u001b[39m callbacks = [\n\u001b[32m     46\u001b[39m     tf.keras.callbacks.EarlyStopping(\n\u001b[32m     47\u001b[39m         monitor=\u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, patience=\u001b[32m6\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     )\n\u001b[32m     52\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_tr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_va\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# ========= 3) Evaluación multiclase =========\u001b[39;00m\n\u001b[32m     66\u001b[39m val_loss, val_acc = model.evaluate((X_va, M_va), y_va, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, balanced_accuracy_score\n",
    "\n",
    "# ========= 1) Datos multiclase =========\n",
    "W_KEY = \"upto_12\"  # cambia a upto_18 / upto_24 para comparar\n",
    "X = sequences[W_KEY]          # (N, W, F)\n",
    "M = masks[W_KEY]              # (N, W)\n",
    "y = labels[W_KEY].astype(int) # 0,1,2,3\n",
    "\n",
    "X_tr, X_va, M_tr, M_va, y_tr, y_va = train_test_split(\n",
    "    X, M, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# class weights para evitar colapso a \"Pass\"\n",
    "classes = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "class_weight = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "print(\"class_weight:\", class_weight)\n",
    "\n",
    "# ========= 2) Modelo (paper-inspired, multiclase) =========\n",
    "model = GLUTransformerClassifier(\n",
    "    latent_d=512,      # tamaño hidden \"paper-like\"\n",
    "    num_heads=8,       # 512 % 8 = 0\n",
    "    ff_dim=512,        # puedes subir a 1024/2048 luego\n",
    "    dropout=0.1,\n",
    "    num_classes=4,     # multiclase\n",
    "    num_layers=2,      # 2 encoders\n",
    "    max_len=128\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=6, restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=(X_tr, M_tr),\n",
    "    y=y_tr,\n",
    "    validation_data=((X_va, M_va), y_va),\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    class_weight=class_weight,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========= 3) Evaluación multiclase =========\n",
    "val_loss, val_acc = model.evaluate((X_va, M_va), y_va, verbose=0)\n",
    "y_proba = model.predict((X_va, M_va), verbose=0)\n",
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "\n",
    "macro_f1 = f1_score(y_va, y_pred, average=\"macro\")\n",
    "weighted_f1 = f1_score(y_va, y_pred, average=\"weighted\")\n",
    "bal_acc = balanced_accuracy_score(y_va, y_pred)\n",
    "\n",
    "print(f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "print(f\"macro_f1={macro_f1:.4f} weighted_f1={weighted_f1:.4f} balanced_acc={bal_acc:.4f}\")\n",
    "print(classification_report(y_va, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_va, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
