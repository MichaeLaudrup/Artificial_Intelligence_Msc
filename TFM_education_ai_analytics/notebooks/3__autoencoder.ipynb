{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d68b49",
   "metadata": {},
   "source": [
    "Esto es una pprueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16ede0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 15:38:41.746435: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GPU DETECTADA: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "ğŸš€ VersiÃ³n de TensorFlow: 2.20.0\n",
      "ğŸ“ RaÃ­z del proyecto aÃ±adida: /workspace/TFM_education_ai_analytics\n",
      "âœ¨ Celda de configuraciÃ³n completada.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ============================================================\n",
    "# 1. CONFIGURACIÃ“N DE GPU\n",
    "# ============================================================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Configurar para que no reserve toda la VRAM de golpe\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… GPU DETECTADA: {gpus}\")\n",
    "        print(f\"ğŸš€ VersiÃ³n de TensorFlow: {tf.__version__}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ Error configurando GPU: {e}\")\n",
    "else:\n",
    "    print(\"âŒ NO SE DETECTÃ“ GPU. El entrenamiento serÃ¡ mÃ¡s lento.\")\n",
    "\n",
    "# ============================================================\n",
    "# 2. CONFIGURACIÃ“N DE RUTAS Y CÃ“DIGO LOCAL\n",
    "# ============================================================\n",
    "# AÃ±adimos la raÃ­z del proyecto al path para poder importar 'educational_ai_analytics'\n",
    "root_path = os.path.abspath(\"..\")\n",
    "if root_path not in sys.path:\n",
    "    sys.path.insert(0, root_path)\n",
    "\n",
    "# Limpiamos cachÃ© del mÃ³dulo para asegurar que cargamos los Ãºltimos cambios del .py\n",
    "if 'educational_ai_analytics' in sys.modules:\n",
    "    del sys.modules['educational_ai_analytics']\n",
    "if 'educational_ai_analytics.modeling' in sys.modules:\n",
    "    del sys.modules['educational_ai_analytics.modeling']\n",
    "\n",
    "print(f\"ğŸ“ RaÃ­z del proyecto aÃ±adida: {root_path}\")\n",
    "print(\"âœ¨ Celda de configuraciÃ³n completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df41b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Cargando features desde: ../data/processed/training/features\n",
      "âœ… Estudiantes cargados: 22785\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š Tensor Temporal (X_train_temp):   (22785, 40, 11)\n",
      "ğŸ“Š Tensor EstÃ¡tico (X_train_static): (22785, 10)\n",
      "âœ… Rango 0-1 verificado: 0.0 a 1.0\n",
      "âœ¨ Datos listos para el entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3. CARGA DE DATOS Y CONSTRUCCIÃ“N DE TENSORES\n",
    "# ============================================================\n",
    "DATA_DIR = Path(\"../data/processed/training/features\")\n",
    "\n",
    "def prepare_tensors(path: Path):\n",
    "    print(f\"ğŸš€ Cargando features desde: {path}\")\n",
    "    \n",
    "    # Cargamos los CSVs asegurando alineaciÃ³n total mediante sort_index()\n",
    "    # (Si no se ordenan, el alumno de la fila 1 de clics podrÃ­a ser el de la fila 50 de static)\n",
    "    clicks = pd.read_csv(path / \"ts_clicks.csv\", index_col=0).sort_index()\n",
    "    perf = pd.read_csv(path / \"ts_performance.csv\", index_col=0).sort_index()\n",
    "    proc = pd.read_csv(path / \"ts_procrastination.csv\", index_col=0).sort_index()\n",
    "    static = pd.read_csv(path / \"static_features.csv\", index_col=0).sort_index()\n",
    "    \n",
    "    n_students = len(clicks)\n",
    "    print(f\"âœ… Estudiantes cargados: {n_students}\")\n",
    "\n",
    "    def to_3d_tensor(df, num_vars):\n",
    "        # Convierte Tabla Ancha (N, Vars*40) -> Tensor 3D (N, 40, Vars)\n",
    "        return df.values.reshape(n_students, num_vars, 40).transpose(0, 2, 1)\n",
    "\n",
    "    # Transformamos cada bloque a 3D\n",
    "    tensor_clicks = to_3d_tensor(clicks, 4)\n",
    "    tensor_perf = to_3d_tensor(perf, 6)\n",
    "    tensor_proc = to_3d_tensor(proc, 1)\n",
    "\n",
    "    # Concatenamos todas las variables temporales (Total: 4+6+1 = 11 variables)\n",
    "    X_temp_raw = np.concatenate([tensor_clicks, tensor_perf, tensor_proc], axis=2)\n",
    "    X_static = static.values\n",
    "    \n",
    "    return X_temp_raw, X_static\n",
    "\n",
    "# 1. Ejecutamos la carga limpia desde el disco\n",
    "X_train_temp_raw, X_train_static = prepare_tensors(DATA_DIR)\n",
    "\n",
    "# 2. NORMALIZACIÃ“N GLOBAL (Clave para que PCA y AE compitan igual)\n",
    "# Aplicamos MinMaxScaler a las 11 variables temporales a la vez\n",
    "N, T, F = X_train_temp_raw.shape\n",
    "scaler = MinMaxScaler()\n",
    "# Aplanamos, entrenamos el scaler y volvemos a dar forma 3D\n",
    "X_train_temp = scaler.fit_transform(X_train_temp_raw.reshape(-1, F)).reshape(N, T, F)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"ğŸ“Š Tensor Temporal (X_train_temp):   {X_train_temp.shape}\")\n",
    "print(f\"ğŸ“Š Tensor EstÃ¡tico (X_train_static): {X_train_static.shape}\")\n",
    "print(f\"âœ… Rango 0-1 verificado: {X_train_temp.min()} a {X_train_temp.max()}\")\n",
    "print(f\"âœ¨ Datos listos para el entrenamiento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a1f65",
   "metadata": {},
   "source": [
    "### BASELINE: PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0933a3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Usando latent_dim de params.py: 32\n",
      "------------------------------------------------------------\n",
      "ğŸ“‰ MSE del PCA (32 dims): 0.005180\n",
      "ğŸ“ˆ Varianza explicada total: 0.9048\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. BASELINE: PCA (ReducciÃ³n Lineal)\n",
    "# ============================================================\n",
    "from educational_ai_analytics.modeling.params import AE_PARAMS\n",
    "\n",
    "# Sacamos la dimensiÃ³n latente directamente de tu archivo central de parÃ¡metros\n",
    "latent_dim = AE_PARAMS.latent_dim\n",
    "\n",
    "# Preparamos los datos aplanados para el PCA\n",
    "X_train_flat = X_train_temp.reshape(X_train_temp.shape[0], -1)\n",
    "X_pca_input = np.concatenate([X_train_flat, X_train_static], axis=1)\n",
    "\n",
    "print(f\"ğŸ“ Usando latent_dim de params.py: {latent_dim}\")\n",
    "\n",
    "# Entrenamos PCA\n",
    "pca = PCA(n_components=latent_dim, random_state=42)\n",
    "latent_pca = pca.fit_transform(X_pca_input)\n",
    "reconstructed_pca = pca.inverse_transform(latent_pca)\n",
    "\n",
    "# Calculamos el MSE\n",
    "mse_pca = np.mean(np.square(X_pca_input - reconstructed_pca))\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"ğŸ“‰ MSE del PCA ({latent_dim} dims): {mse_pca:.6f}\")\n",
    "print(f\"ğŸ“ˆ Varianza explicada total: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70584938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1770997129.208547   67423 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5580 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-13 15:38:54.727763: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 0.0257 - val_loss: 0.0131 - learning_rate: 0.0010\n",
      "Epoch 2/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0121 - val_loss: 0.0103 - learning_rate: 0.0010\n",
      "Epoch 3/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0104 - val_loss: 0.0097 - learning_rate: 0.0010\n",
      "Epoch 4/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0096 - val_loss: 0.0089 - learning_rate: 0.0010\n",
      "Epoch 5/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0091 - val_loss: 0.0086 - learning_rate: 0.0010\n",
      "Epoch 6/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0088 - val_loss: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 7/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0085 - val_loss: 0.0079 - learning_rate: 0.0010\n",
      "Epoch 8/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0082 - val_loss: 0.0078 - learning_rate: 0.0010\n",
      "Epoch 9/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0079 - val_loss: 0.0074 - learning_rate: 0.0010\n",
      "Epoch 10/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0076 - val_loss: 0.0072 - learning_rate: 0.0010\n",
      "Epoch 11/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0074 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 12/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0072 - val_loss: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 13/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0071 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 14/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0069 - val_loss: 0.0067 - learning_rate: 0.0010\n",
      "Epoch 15/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0068 - val_loss: 0.0069 - learning_rate: 0.0010\n",
      "Epoch 16/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0068 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 17/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0067 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 18/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0066 - val_loss: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 19/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0065 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 20/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0064 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 21/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0064 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 22/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0063 - val_loss: 0.0061 - learning_rate: 0.0010\n",
      "Epoch 23/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0063 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 24/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0062 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 25/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0061 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 26/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0061 - val_loss: 0.0059 - learning_rate: 0.0010\n",
      "Epoch 27/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0059 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 28/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0059 - val_loss: 0.0058 - learning_rate: 0.0010\n",
      "Epoch 29/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0059 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 30/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0057 - val_loss: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 31/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0057 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 32/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0056 - val_loss: 0.0056 - learning_rate: 0.0010\n",
      "Epoch 33/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0055 - val_loss: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 34/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0055 - val_loss: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 35/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0055 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 36/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0054 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 37/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0053 - val_loss: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 38/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0053 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 39/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0052 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 40/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0053 - val_loss: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 41/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0052 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 42/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 39ms/step - loss: 0.0051 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 43/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0051 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 44/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0050 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 45/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0050 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 46/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0051 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 47/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0049 - val_loss: 0.0050 - learning_rate: 0.0010\n",
      "Epoch 48/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0049 - val_loss: 0.0052 - learning_rate: 0.0010\n",
      "Epoch 49/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0049 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 50/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0050 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 51/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0049 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 52/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0049 - val_loss: 0.0051 - learning_rate: 0.0010\n",
      "Epoch 53/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0048 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 54/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0049 - val_loss: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 55/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0051 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 56/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0048 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 57/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0047 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 58/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0047 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 59/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0047 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 60/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0049 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 61/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0046 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 62/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0046 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 63/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0046 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 64/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0046 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 65/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0045 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 66/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0045 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 67/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0045 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 68/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - loss: 0.0045 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 69/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0045 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 70/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0044 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 71/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0047 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 72/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0044 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 73/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0044 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 74/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 75/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0045 - val_loss: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 76/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 35ms/step - loss: 0.0045 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 77/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 78/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0043 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 79/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0045 - val_loss: 0.0044 - learning_rate: 0.0010\n",
      "Epoch 80/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0042 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 81/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0045 - val_loss: 0.0047 - learning_rate: 0.0010\n",
      "Epoch 82/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0043 - val_loss: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 83/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 84/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0042 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 85/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 86/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0044 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 87/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 88/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 89/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0043 - val_loss: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 90/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0043 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 91/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 92/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0041 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 93/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 94/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0010\n",
      "Epoch 95/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0041 - val_loss: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 96/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0040 - val_loss: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 97/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0040 - val_loss: 0.0046 - learning_rate: 0.0010\n",
      "Epoch 98/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0045\n",
      "Epoch 98: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0044 - val_loss: 0.0070 - learning_rate: 0.0010\n",
      "Epoch 99/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0041 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 100/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 101/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 102/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 103/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 104/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 105/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 106/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 107/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 5.0000e-04\n",
      "Epoch 108/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0043 - val_loss: 0.0042 - learning_rate: 5.0000e-04\n",
      "Epoch 109/150\n",
      "\u001b[1m142/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0041\n",
      "Epoch 109: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 5.0000e-04\n",
      "Epoch 110/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 2.5000e-04\n",
      "Epoch 111/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 2.5000e-04\n",
      "Epoch 112/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 2.5000e-04\n",
      "Epoch 113/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 114/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 115/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 116/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 117/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 2.5000e-04\n",
      "Epoch 118/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 119/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 120/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 34ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 121/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0038\n",
      "Epoch 121: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 2.5000e-04\n",
      "Epoch 122/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 123/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 124/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 125/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 126/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 127/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 33ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 128/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 129/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 130/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 131/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 132/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 133/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 134/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 135/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 136/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.2500e-04\n",
      "Epoch 137/150\n",
      "\u001b[1m142/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0037\n",
      "Epoch 137: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 1.2500e-04\n",
      "Epoch 138/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 139/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 140/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 141/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 142/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 143/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 40ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 144/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 145/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 146/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 147/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 148/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 149/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n",
      "Epoch 150/150\n",
      "\u001b[1m143/143\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. ENTRENAMIENTO DINÃMICO DESDE PARAMS.PY\n",
    "# ============================================================\n",
    "from educational_ai_analytics.modeling import TimeSeriesAutoencoder\n",
    "from educational_ai_analytics.modeling.params import AE_PARAMS\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Instanciamos usando TODA la configuraciÃ³n centralizada\n",
    "model = TimeSeriesAutoencoder(\n",
    "    latent_dim=AE_PARAMS.latent_dim,\n",
    "    timesteps=AE_PARAMS.timesteps,\n",
    "    temp_features=AE_PARAMS.temp_features,\n",
    "    static_features=AE_PARAMS.static_features,\n",
    "    lstm_units=AE_PARAMS.lstm_units,\n",
    "    dense_static_units=AE_PARAMS.dense_static_units,\n",
    "    activation=AE_PARAMS.activation\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=AE_PARAMS.learning_rate), \n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "# Callbacks configurados desde params.py\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        \"../models/best_autoencoder.keras\", \n",
    "        save_best_only=True, \n",
    "        monitor='val_loss'\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        patience=AE_PARAMS.early_stopping_patience, \n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=AE_PARAMS.reduce_lr_factor, \n",
    "        patience=AE_PARAMS.reduce_lr_patience, \n",
    "        min_lr=AE_PARAMS.min_learning_rate,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=[X_train_temp, X_train_static],\n",
    "    y=X_train_temp, \n",
    "    epochs=AE_PARAMS.epochs,\n",
    "    batch_size=AE_PARAMS.batch_size,\n",
    "    validation_split=AE_PARAMS.validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. EXTRACCIÃ“N Y GUARDADO DE EMBEDDINGS (ESPACIO LATENTE)\n",
    "# ============================================================\n",
    "import os\n",
    "\n",
    "# 1. Crear directorio\n",
    "output_path = \"../data/processed/training/embeddings\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# 2. Obtener IDs de alumnos (los guardamos al cargar en la Celda 2)\n",
    "# Si perdiste el Ã­ndice, cÃ¡rgalo de uno de los CSVs originales\n",
    "student_ids = pd.read_csv(DATA_DIR / \"static_features.csv\", index_col=0).index\n",
    "\n",
    "# 3. Extraer espacio latente del Autoencoder\n",
    "# Usamos la funciÃ³n .encode() que definimos en el modelo\n",
    "latent_ae_values = model.encode([X_train_temp, X_train_static]).numpy()\n",
    "\n",
    "# 4. Crear DataFrames con los IDs como Ã­ndice\n",
    "df_latent_ae = pd.DataFrame(\n",
    "    latent_ae_values, \n",
    "    index=student_ids, \n",
    "    columns=[f\"ae_dim_{i}\" for i in range(AE_PARAMS.latent_dim)]\n",
    ")\n",
    "\n",
    "df_latent_pca = pd.DataFrame(\n",
    "    latent_pca, \n",
    "    index=student_ids, \n",
    "    columns=[f\"pca_dim_{i}\" for i in range(AE_PARAMS.latent_dim)]\n",
    ")\n",
    "\n",
    "# 5. Guardar en CSV\n",
    "df_latent_ae.to_csv(f\"{output_path}/latent_ae.csv\")\n",
    "df_latent_pca.to_csv(f\"{output_path}/latent_pca.csv\")\n",
    "\n",
    "print(f\"âœ… Embeddings guardados en: {output_path}\")\n",
    "print(f\"ğŸ“Š Dimensiones AE:  {df_latent_ae.shape}\")\n",
    "print(f\"ğŸ“Š Dimensiones PCA: {df_latent_pca.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
